{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple GP \n",
    "\n",
    "Here we will put together a simple GP using the equations given in the lectures.  I have given all the equations you will need in the notes below.  You should note that this implementation of a GP is slow, $\\mathcal{O}(n^{3})$.  For this reason we will avoid exploring the parameter space in an extensive and systematic way.  You should however, play around. Do your own exploration of the impact of the hyperparameters passed to the kernel.\n",
    "\n",
    "Having said that this implementation is slow, you will see that it is substantially faster than the work we did in the kernal sandbox notebook.\n",
    "\n",
    "Let's import the libraries we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now implement your kernel function in the same way as in the kernel sandbox notebook.  For now start with a squared exponential kernel function but feel free to change this as you become more familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k(x_i, x_j) = \\sigma^2 \\exp \\left(\\frac{-1}{2 l^2} (x_i - x_j)^2 \\right)$\n",
    "\n",
    "Below is my code for the squared exponential kernel.  Either keep this code, or replace it with the kernel function you wrote in the kernel sandbox notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel(x, y, p):\n",
    "    ''' Returns a sqaured exponetial covariance matrix '''\n",
    "    # p[0] = sigma\n",
    "    # p[1] = length scale\n",
    "    return p[0]**2 * np.exp(-0.5 * np.subtract.outer(x, y)**2 / p[1]**2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set the mean function to be zero ($m({\\bf t}) = 0$) then we now have enough information to completely define our Gaussian process.\n",
    "\n",
    "I will now define some observed data.  The data has been collected at time $t_1$ and has been observed to be $y_1$ with very little (we will ignore it) uncertainty.  Below I make an array (actually a Python list) of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1= [1.0]\n",
    "y_1 = [0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Process\n",
    "\n",
    "If we start with the 'classic' GP probability equation:\n",
    "\n",
    "$p({\\bf y_1, y_2}) = \\mathcal{N}\\left( \\begin{bmatrix} {\\bf a} \\\\ {\\bf c} \\end{bmatrix},  \\begin{bmatrix} A & B \\\\ B^{T} & C \\end{bmatrix} \\right)$\n",
    "\n",
    "and I'll remind you of what $A, B, C$ really mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some observations $y_1$ taken at $t_1$ and there are some predictions we want to make of $y_2$ and times $t_2$.\n",
    "\n",
    "$A$ is the covariance matrix that describes the covariance of the data.  That is the covariance of the points in $t_1$ with the other points in $t_1$, where $t_1$ has length $n$.\n",
    "\n",
    "$A = \\begin{bmatrix} {\\rm var}(y_{1,0}) & {\\rm cov}(y_{1,0}, y_{1,1}) & ... & {\\rm cov}(y_{1,0}, y_{1,n}) \\\\ \n",
    "                     {\\rm cov}(y_{1,1}, y_{1,0}) & {\\rm var}(y_{1,1}) & ... & {\\rm cov}(y_{1,1}, y_{1,n}) \\\\\n",
    "                     ... & ... & ... & ... \\\\\n",
    "                     {\\rm cov}(y_{1,n}, y_{1,0}) & ... & ... & {\\rm var}(y_{1,n}) \\end{bmatrix}$\n",
    "                     \n",
    "$C$ is the covariance matrix that describes the covariance of the prediction.  That is the covariance of the points in $t_2$ with the other points in $t_2$, where $t_2$ has length $m$.\n",
    "\n",
    "$C = \\begin{bmatrix} {\\rm var}(y_{2,0}) & {\\rm cov}(y_{2,0}, y_{2,1}) & ... & {\\rm cov}(y_{2,0}, y_{2,m}) \\\\ \n",
    "                     {\\rm cov}(y_{2,1}, y_{2,0}) & {\\rm var}(y_{2,1}) & ... & {\\rm cov}(y_{2,1}, y_{2,m}) \\\\\n",
    "                     ... & ... & ... & ... \\\\\n",
    "                     {\\rm cov}(y_{2,m}, y_{2,0}) & ... & ... & {\\rm var}(y_{2,m}) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$B$ is the covariance matrix that describes the covariance of the prediction and the data.  That is the covariance of the points in $t_1$ with the points in $t_2$.\n",
    "\n",
    "$B = \\begin{bmatrix} {\\rm cov}(y_{1,0}, y_{2,0}) & {\\rm cov}(y_{1,0}, y_{2,1}) & ... & {\\rm cov}(y_{1,0}, y_{2,n}) \\\\ \n",
    "                     {\\rm cov}(y_{1,1}, y_{2,0}) & {\\rm cov}(y_{1,1}, , y_{2,1}) & ... & {\\rm cov}(y_{1,1}, y_{2,n}) \\\\\n",
    "                     ... & ... & ... & ... \\\\\n",
    "                     {\\rm cov}(y_{1,m}, y_{2,0}) & ... & ... & {\\rm cov}(y_{1,m}, y_{2,n}) \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the python function we have writen, we can easily calculate $A, B, C$.  We denote the hyperparameters passed to the kernel function as $\\theta$.\n",
    "\n",
    "$A = K(t_1, t_1, \\theta)$\n",
    "\n",
    "$C = K(t_2, t_2, \\theta)$\n",
    "\n",
    "$B = K(t_1, t_2, \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The likelihood function\n",
    "\n",
    "We defined the multivariate normal probability distribution in the lectures:\n",
    "\n",
    "$p({\\bf y}) = \\operatorname{det}(2\\pi \\Sigma)^{-\\frac{1}{2}} \\, \\exp \\left( { -\\frac{1}{2}(\\mathbf{y - \\mu}) \\Sigma^{-1}(\\mathbf{y - \\mu})^{T} } \\right)$,\n",
    "\n",
    "and can use the more compact notation in the context of a $\\mathcal{GP}$:\n",
    "\n",
    "$p({\\bf y_1, y_2}) = \\mathcal{N}\\left( \\begin{bmatrix} {\\bf a} \\\\ {\\bf c} \\end{bmatrix},  \\begin{bmatrix} A & B \\\\ B^{T} & C \\end{bmatrix} \\right)$,\n",
    "\n",
    "or \n",
    "\n",
    "$\\begin{bmatrix} {\\bf y_1} \\\\ {\\bf y_2} \\end{bmatrix} \\sim \\mathcal{N}\\left( \\begin{bmatrix} {\\bf a} \\\\ {\\bf c} \\end{bmatrix},  \\begin{bmatrix} A & B \\\\ B^{T} & C \\end{bmatrix} \\right)$.\n",
    "\n",
    "When we have some data $y_1$ we recall that the likelihood function is:\n",
    "\n",
    "$p({\\bf y_1}) = \\int p({\\bf y_2}, {\\bf y_1}) \\,  {\\rm d}{\\bf y_2}$,\n",
    "\n",
    "so \n",
    "\n",
    "$p({\\bf y_1}) = \\mathcal{N}({\\bf a}, A)$.\n",
    "\n",
    "Which is even more straight forward when we set the mean function to be zero:\n",
    "\n",
    "$p({\\bf y_1}) = \\mathcal{N}({\\bf 0}, A)$.\n",
    "\n",
    "And expanding this out so you have if for reference we have:\n",
    "\n",
    "$p({\\bf y_1}) = \\operatorname{det}(2\\pi A)^{-\\frac{1}{2}} \\, \\exp \\left( { -\\frac{1}{2}\\mathbf{y_1} A^{-1}\\mathbf{y_1}^{T}}  \\right)$,\n",
    "\n",
    "or\n",
    "\n",
    "$\\log p({\\bf y_1}) = -\\frac{1}{2} {\\bf y_1}^{T} A^{-1} {\\bf y_1} - \\frac{1}{2} \\log|A| - \\frac{n}{2} \\log 2\\pi$.\n",
    "\n",
    "Write a function that accepts as arguments the data $\\bf y_1$ and the covariance matric $A$ and returns $p({\\bf y_1})$. (or returns $\\log p({\\bf y_1})$ )\n",
    "\n",
    "Note that numpy has a linear algebra library that will allow you to calculate the determinant (e.g., np.linalg.det(A)) and the inverse (e.g., np.linalg.inv(A)).  Also, matrix multiplications can be made using np.dot(y1, A) or even np.dot(y1, A).dot(y1.T)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Making predictions using a GP is simply a case of using the conditional distribution:\n",
    "\n",
    "$p({\\bf y_2} | {\\bf y_1}) = \\frac{p({\\bf y_1, y_2})}{p({\\bf y_1})}$,\n",
    "\n",
    "where\n",
    "\n",
    "$p({\\bf y_1, y_2}) = \\mathcal{N}\\left( \\begin{bmatrix} {\\bf a} \\\\ {\\bf c} \\end{bmatrix},  \\begin{bmatrix} A & B \\\\ B^{T} & C \\end{bmatrix} \\right)$,\n",
    "\n",
    "and \n",
    "\n",
    "$p({\\bf y_1}) = \\mathcal{N}({\\bf a}, A)$.\n",
    "\n",
    "If we include non-zero mean terms then this becomes:\n",
    "\n",
    "$p({\\bf y_2} | {\\bf y_1}) = \\mathcal{N}({\\bf c} + BA^{-1} ({\\bf y_1} - {\\bf a}), C - BA^{-1}B^{T})$.\n",
    "\n",
    "Or with the mean terms set to zero this becomes:\n",
    "\n",
    "$p({\\bf y_2} | {\\bf y_1}) = \\mathcal{N}(BA^{-1} ({\\bf y_1}), C - BA^{-1}B^{T})$.\n",
    "\n",
    "Write a function that accepts as arguments: $\\bf t_2$; $\\bf t_1$; the kernel function; the kernel parameters $\\theta$; and the observed data points $\\bf y_1$.  This function should return the predicted mean values and the predicted variance for the points in $\\bf t_2$.\n",
    "\n",
    "The function will take the following form ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(t_2, t_1, kernel, theta, y_1):\n",
    "    B = kernel(t_1, t_2, theta)\n",
    "    #YOUR CODE HERE\n",
    "    #MORE OF YOUR CODE\n",
    "    #...\n",
    "    #MORE OF YOUR CODE\n",
    "    y_pred = 1 # Delete this!\n",
    "    sigma_new = 1 # Delete this!\n",
    "    return y_pred, sigma_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run your code to make predictions at time $\\bf t_2$.  Run the following code below:\n",
    "\n",
    "(If something goes wrong try to find the reason using the returned error message.  If you are stuck ask for help!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1= [1.0]\n",
    "y_1 = [0.1]\n",
    "t_2 = np.linspace(0, 2, 200)\n",
    "theta = [1.0, 0.1]\n",
    "y_pred, sigmas = predict(t_2, t_1, kernel, theta, y_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the results.  The code below is an example of a way to plot the predictions together with the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(t_2, y_pred, yerr=sigmas**0.5, capsize=0, label='Prediciton Interval')\n",
    "ax.plot(t_1, y_1, 'ro', label='Observation')\n",
    "ax.plot(t_2, y_pred, 'b--', lw=4, label='Prediction')\n",
    "ax.legend()\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add more data.  Run the code below and then write your own code to plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1= [1.0]\n",
    "y_1 = [0.1]\n",
    "t_1.append(1.3)\n",
    "y_1.append(0.3)\n",
    "y_pred, sigmas = predict(t_2, t_1, kernel, theta, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's again add more data.  Run the code below and then write your own code to plot the results.  The impact of the observations should be obvious.  If you are are happy with what is going on, start to vary the properties of the kernel (the hyperparameters).  If you are really happy, see what happens when the data are generated using hyperparameters that are different to the hyperparameters used for the prediction.\n",
    "\n",
    "If you explored different kernels in the snadbox notebook, then consider implemeting these kernels here.\n",
    "\n",
    "Once you are comfortable move on to the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = [0.4, 0.7, 1.1, 2.3, 2.9, 3.0]\n",
    "theta = [1.0, 0.3]\n",
    "y_1 = np.random.multivariate_normal(np.zeros(len(t_1)), kernel(t_1, t_1, theta))\n",
    "t_2 = np.linspace(0, 4, 1000)\n",
    "y_pred, sigmas = predict(t_2, t_1, kernel, theta, y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The last step\n",
    "\n",
    "As a last step in this notebook I would like you to explore how we might choose (or estimate even) sensible hyperparameters.  \n",
    "\n",
    "We will use a simple 2D grid search.  For each $l$ in 'ls' and each $\\sigma$ in 'sigmas' calculate the likelihood (or log likelihood) using the function that you wrote above.  Store each result in 'L[i, j]' and then use the cell below to plot the results.  (note if you are using the log likelihood already you will not need to take the log again).\n",
    "\n",
    "We will not explore extensively and discussion of the uncertainties are beyond the scope of this exercise.  But it is worth you considering what the plot of the likelihood as a function of the hyperparameters means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = [0.4, 0.7, 1.1, 2.3, 2.9, 3.0]\n",
    "theta = [1.0, 1.0]\n",
    "y_1 = np.random.multivariate_normal(np.zeros(len(t_1)), kernel(t_1, t_1, theta))\n",
    "nps = 300\n",
    "ls = np.linspace(0.01,2,nps)\n",
    "sigmas = np.linspace(0.1,5.0,nps)\n",
    "L = np.zeros([nps, nps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "SIGMAS, LS = np.meshgrid(sigmas, ls)\n",
    "levels = np.arange(- 20.0, L.max(), 1)\n",
    "CS = ax.contour(SIGMAS, LS, np.log(L), levels=levels)\n",
    "cbar = fig.colorbar(CS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now pick the 'best' hyperparamters from the plot above and plot the predictions using the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to the last notebook.  In the final notebook we will use Celerite to take care of all of the lower level computations we have managed here.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
