{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import celerite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celerite example\n",
    "\n",
    "## A planet around a star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to give an introduction to the use of Celerite and also to give an example of it's application.  We will leave all of the $\\mathcal{GP}$ maths behind us in the previous notebook and focus on the application.\n",
    "\n",
    "We will look at an example (not a very realistic example) of a planet around a star.  The planet creates a radial velocity signal that is deterministic, while the star creates a correlated noise.  Our aim is to estimate the parameters of the radial velocity signal.\n",
    "\n",
    "In order to keep things as stright forward as possible we will make the RV signal as (no phase, no eccentricity, no ... - it is simple):\n",
    "\n",
    "$f(t) = A \\sin \\left( \\frac{2 \\pi t}{P}\\right)$.\n",
    "\n",
    "From Celerite we will import the Model class from which our own model will inherit properties.  We define the class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celerite.modeling import Model\n",
    "\n",
    "# Define the model\n",
    "class MeanModel(Model):\n",
    "    parameter_names = (\"A\", \"P\")\n",
    "\n",
    "    def get_value(self, t):\n",
    "        return self.A * np.sin(2.0 * np.pi * t / self.P)\n",
    "    \n",
    "model = MeanModel(A = 10.0, P = 340.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make some data\n",
    "\n",
    "Next we will make some data.  here we use a kernel function as you will have coded in previous examples.  We set up some observations by defining the time array, generating some correlated noise from the kernel, adding the RV model, and then adding some white noise (a small amount of white noise). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(53)\n",
    "A = 10.0\n",
    "P = 340.0\n",
    "model = MeanModel(A = A, P = P)\n",
    "theta = [5.0, 250.0]\n",
    "npts = 150\n",
    "yerr = np.ones(npts) * 0.1\n",
    "t = np.linspace(0, 2000, npts)\n",
    "\n",
    "def kernel(x, y, p):\n",
    "    # p[0] = sigma\n",
    "    # p[1] = length scale\n",
    "    k = p[0]**2 * np.exp(-0.5 * np.subtract.outer(x, y)**2 / p[1]**2)\n",
    "    return k\n",
    "\n",
    "y = np.random.multivariate_normal(np.zeros(len(t)), kernel(t, t, theta), 1)[0]\n",
    "y += model.get_value(t)\n",
    "y += np.random.randn(len(y)) * yerr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now plot the data we will see the difference between the RV signal we wish to recover and the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(t, y, yerr=yerr, alpha=0.5, label='Observations')\n",
    "ax.plot(t, model.get_value(t), label='RV signal')\n",
    "ax.legend()\n",
    "ax.set_xlabel('t')\n",
    "ax.set_ylabel('v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set is comprised of three components.  A sinusodial planetary signal; correlated noise of stellar origin; uncorrelated 'white' intrumental noise.  \n",
    "\n",
    "It is straightforward to change the properties of any of the components above and see the impact in the observations.  If you do not have a good feel for the properties of the correlated noise please play with the properties as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's fit the Data the usual way\n",
    "\n",
    "For now, let's model the data on the assumption that the noise is white.  In order to do this I will make use of the emcee hammer (http://dfm.io/emcee/current/). A detailed explaination of the code is far beyond the scope and the timescale of this exercise but suffice to say the code is an MCMC ensemble sampler.  Please follow the link and ask questions if you know nothing of MCMC.\n",
    "\n",
    "In order to run this code but keep the footprint of the code to a minimum I have written a like class below.  I have neglected proper priors and kept only the essentials.  This hopefully has the advantage that it is more readable.\n",
    "\n",
    "After the definition of the class I import the emcee library.  In a few lines of code wemake an instance of the like class (lp), setup some start conditions, and create a smapler instance.\n",
    "\n",
    "We then 'burn-in' the sampler for an arbitrary number of steps (proper tests for convergence should be used here) and then finally reset the ssmpler befor sampling from what we hope to be the posterior probability distribution.\n",
    "\n",
    "If all this is a little too much then please ask or refer to the emcee documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class like():\n",
    "    def __init__(self, t, y, yerr):\n",
    "        self.y = y\n",
    "        self.yerr = yerr\n",
    "        self.t = t\n",
    "\n",
    "    def model(self, params):\n",
    "        return params[0] * np.sin(2.0 * np.pi * self.t / params[1])\n",
    "        \n",
    "    def __call__(self, params):\n",
    "        if params[0] < 0:\n",
    "            return -np.inf\n",
    "        return np.sum(-0.5 * (y - self.model(params))**2 / yerr**2  )     \n",
    "            \n",
    "import emcee\n",
    "\n",
    "lp = like(t, y, yerr)\n",
    "initial = np.array([A, P])\n",
    "ndim, nwalkers = len(initial), 32\n",
    "p0 = initial + 1e-8 * np.random.randn(nwalkers, ndim)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lp)\n",
    "\n",
    "print(\"Running burn-in...\")\n",
    "p0, lp, _ = sampler.run_mcmc(p0, 2000)\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(p0, 2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have samples stored in the sampler that describe the posterior probability density function for the likelihood (and prior function) that we have defined.  We use the corner plot routine to examine the marginalised 1D posteriors and the covariance between both the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "corner.corner(sampler.flatchain, labels=['A', 'P'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look back at the true parameters we input into the simulated data, you will see that the truth is a long way outside of the credible regions for both parameters.  Do you know why this is?\n",
    "\n",
    "For completeness we will plot the data and the model from this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data.\n",
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(t, y, yerr=yerr, fmt=\".k\", capsize=0, label='Data')\n",
    "lp = like(t, y, yerr)\n",
    "# Plot 24 posterior samples.\n",
    "samples = sampler.flatchain\n",
    "for s in samples[np.random.randint(len(samples), size=53)]:\n",
    "    mu = lp.model(s)\n",
    "    ax.plot(t, mu, color='r', alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_xlabel(r\"$t$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's construct a Celerite model\n",
    "\n",
    "Now let's construct a Celerite model to use a $\\mathcal{GP}$ to model the correlated noise.  Setting up this problem in Celerite is strightforward once you know how.  We have a kernel with a single term (but could add more) together with a mean model as defined before.  Having called the compute method we can already print the log likelihood function for the intial parameters we have used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import celerite\n",
    "from celerite import terms\n",
    "\n",
    "bounds = dict(log_a=(-5, 10), log_c=(-10, 5))\n",
    "kernel = terms.RealTerm(log_a=np.log(np.var(y)), log_c=-np.log(10.0), bounds=bounds)\n",
    "gp = celerite.GP(kernel, mean=model, fit_mean=True)\n",
    "gp.compute(t, yerr)  # You always need to call compute once.\n",
    "print(\"Initial log likelihood: {0}\".format(gp.log_likelihood(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to persuade Celerite to talk to emcee, we create a function that returns the log likelihood plus the log prior probability.  In the function we set the gp parameters to params, get the log prior from the gp, and the return the log likelihood plut lp.\n",
    "\n",
    "Note the if statements that save us computational time.  If the prior is not finite then there is little value to calculating the log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_probability(params):\n",
    "    gp.set_parameter_vector(params)\n",
    "    lp = gp.log_prior()\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    if params[2] < 0:\n",
    "        return -np.inf\n",
    "    return gp.log_likelihood(y) + lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we now setup emcee and run.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "initial = gp.get_parameter_vector()\n",
    "ndim, nwalkers = len(initial), 32\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability)\n",
    "\n",
    "print(\"Running burn-in...\")\n",
    "p0 = initial + 1e-8 * np.random.randn(nwalkers, ndim)\n",
    "p0, lp, _ = sampler.run_mcmc(p0, 2000)\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(p0, 2000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now show the corner plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(sampler.flatchain, labels=['log a', 'log c', 'A', 'P'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the much better agreement between the estimates of the RV parameters and the truth values.  But we can also see that we have estimated the hyperparameters.  \n",
    "\n",
    "We can use our samples from the posterior probability distribution to plot both the model for the observed data (including correlated noise) and the model for the true RV signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data.\n",
    "plt.errorbar(t, y, yerr=yerr, fmt=\".k\", capsize=0, label='Data')\n",
    "\n",
    "lp = like(t, y, yerr)\n",
    "# Plot 24 posterior samples.\n",
    "samples = sampler.flatchain\n",
    "for s in samples[np.random.randint(len(samples), size=53)]:\n",
    "    gp.set_parameter_vector(s)\n",
    "    mu = gp.predict(y, t, return_cov=False)\n",
    "    a = plt.plot(t, mu, color='r', alpha=0.3)\n",
    "    mu = lp.model(s[2:])\n",
    "    b = plt.plot(t, mu, color='b', alpha=0.3)\n",
    "\n",
    "plt.ylabel(r\"$y$\")\n",
    "plt.xlabel(r\"$t$\")\n",
    "plt.legend()\n",
    "plt.title(\"posterior predictions\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have made it this far, consider applying a Celerite model to a new data set.  Perhaps you have real data that contains correlated noise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
